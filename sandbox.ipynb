{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webcraping of the website www.pieces-euro.tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the connection from the website\n",
    "url = 'https://www.pieces-euro.tv/'\n",
    "r = requests.get(url)\n",
    "print(r.status_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gets the list of all countries and their respective euro coins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.pieces-euro.tv/allemagne/pieces-euro-berlin-2023', 'https://www.pieces-euro.tv/andorre/pieces-euro-2022', 'https://www.pieces-euro.tv/autriche/pieces-euro-2023', 'https://www.pieces-euro.tv/belgique/pieces-euro-2022', 'https://www.pieces-euro.tv/chypre/pieces-euro-2022', 'https://www.pieces-euro.tv/croatie/pieces-euro-2023', 'https://www.pieces-euro.tv/espagne/pieces-euro-2023', 'https://www.pieces-euro.tv/estonie/pieces-euro-2022', 'https://www.pieces-euro.tv/finlande/pieces-euro-2023', 'https://www.pieces-euro.tv/france/pieces-euro-2023', 'https://www.pieces-euro.tv/grece/pieces-euro-2022', 'https://www.pieces-euro.tv/irlande/pieces-euro-2022', 'https://www.pieces-euro.tv/italie/pieces-euro-2023', 'https://www.pieces-euro.tv/lettonie/pieces-euro-2022', 'https://www.pieces-euro.tv/lituanie/pieces-euro-2023', 'https://www.pieces-euro.tv/luxembourg/pieces-euro-2023', 'https://www.pieces-euro.tv/malte/pieces-euro-2022', 'https://www.pieces-euro.tv/monaco/pieces-euro-2022', 'https://www.pieces-euro.tv/pays-bas/pieces-euro-2023', 'https://www.pieces-euro.tv/portugal/pieces-euro-2023', 'https://www.pieces-euro.tv/saint-marin/pieces-euro-2023', 'https://www.pieces-euro.tv/slovaquie/pieces-euro-2023', 'https://www.pieces-euro.tv/slovenie/pieces-euro-2022', 'https://www.pieces-euro.tv/vatican/pieces-euro-2023', 'https://www.pieces-euro.tv/euro-starter-kit', 'https://www.pieces-euro.tv/pieces-2-euros/2023']\n"
     ]
    }
   ],
   "source": [
    "# Create the soup\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "# find specific 'id' from the soup\n",
    "usermenu = soup.find(id='usermenu')\n",
    "# find in a list all the href all the 'a' tags\n",
    "links = usermenu.find_all('a')\n",
    "# extract only the href of each 'a' tag\n",
    "links = [link.get('href') for link in links]\n",
    "#Create a new list with \"\\n\" characters removed\n",
    "links = [link.replace('\\n', '') for link in links]\n",
    "print(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterates over the list of countries and gets the urls of coins by years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URL format: https://www.pieces-euro.tv/euro-starter-kit\n",
      "Invalid URL format: https://www.pieces-euro.tv/pieces-2-euros/2023\n"
     ]
    }
   ],
   "source": [
    "full_list = []\n",
    "\n",
    "for url in links:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    menuyeartree = soup.find_all(class_='menuyeartree')\n",
    "    menutree = soup.find_all(class_='menutree')\n",
    "    menuactive = soup.find_all(class_='navi_dropdown-content') # get 2023 coins\n",
    "    \n",
    "    href = []\n",
    "    \n",
    "    if menuyeartree:\n",
    "        href = [link.get('href') for menu in menuyeartree for link in menu.find_all('a')]\n",
    "    elif menutree:\n",
    "        href = [link.get('href') for menu in menutree for link in menu.find_all('a')]\n",
    "    elif menuactive:\n",
    "        href = [link.get('href') for menu in menuactive for link in menu.find_all('a')]\n",
    "        \n",
    "        # Increment the URLs in menuactive\n",
    "        href = [urljoin(url, link) for link in href]\n",
    "        \n",
    "        # Add href to full_list\n",
    "        full_list.append(href)\n",
    "        \n",
    "    if href:\n",
    "        year = None\n",
    "        try:\n",
    "            year = int(url.split('-')[-1])\n",
    "        except ValueError:\n",
    "            print(f\"Invalid URL format: {url}\")\n",
    "            continue\n",
    "\n",
    "        if year >= 2001:\n",
    "            new_year = str(year + 1)\n",
    "            new_url = url.replace('-' + str(year) + '-', '-' + new_year + '-')\n",
    "            full_list.append(href)\n",
    "    else:\n",
    "        print('No href found')\n",
    "\n",
    "# print(full_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once the list of urls are obtained, iterates on the list of urls and get all coins infos (price, title, tirage, picture_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_items(full_list: list):\n",
    "    \"\"\"Function to scrape the items from the website\n",
    "\n",
    "    Args:\n",
    "        full_list (list): Lists containing the urls to scrape\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing the information of each item\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for sublist in full_list:\n",
    "        for url in sublist:\n",
    "            r = requests.get(url)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            cat_itembox = soup.find_all(class_='cat_itembox')\n",
    "\n",
    "            if cat_itembox:\n",
    "                for element in cat_itembox:\n",
    "                    price_element = element.find('p', class_='cat_preis')\n",
    "                    title_element = element.find('p', class_='cat_titel')\n",
    "                    tirage_element = element.find('div', class_='cat_info')\n",
    "\n",
    "                    if price_element and title_element and tirage_element:\n",
    "                        price = price_element.text.strip()\n",
    "                        title = title_element.text.strip()\n",
    "                        tirage = tirage_element.text.strip()\n",
    "                        picture_div = element.find('div', class_='cat_preuser')\n",
    "\n",
    "                        if picture_div:\n",
    "                            img = picture_div.find('img')\n",
    "                            if img and 'data-original' in img.attrs:\n",
    "                                picture_url = img['data-original']\n",
    "                                base = 'https://www.pieces-euro.tv/'\n",
    "                                picture_url = urljoin(base, picture_url)\n",
    "                            else:\n",
    "                                picture_url = 'No picture found'\n",
    "                        else:\n",
    "                            picture_url = 'No picture_div found'\n",
    "                        \n",
    "                        # Store the information in a dictionary\n",
    "                        item_info = {\n",
    "                            'price': price,\n",
    "                            'title': title,\n",
    "                            'tirage': tirage,\n",
    "                            'picture_url': picture_url\n",
    "                        }\n",
    "\n",
    "                        results.append(item_info)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in links:\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    menuyeartree = soup.find_all(class_='menuyeartree')\n",
    "    menutree = soup.find_all(class_='menutree')\n",
    "    menuactive = soup.find_all(class_='navi_dropdown-content')\n",
    "\n",
    "    href = []\n",
    "\n",
    "    if menuyeartree:\n",
    "        href = [link.get('href') for menu in menuyeartree for link in menu.find_all('a')]\n",
    "    elif menutree:\n",
    "        href = [link.get('href') for menu in menutree for link in menu.find_all('a')]\n",
    "    elif menuactive:\n",
    "        href = [link.get('href') for menu in menuactive for link in menu.find_all('a')]\n",
    "\n",
    "        # Increment the URLs in menuactive\n",
    "        href = [urljoin(url, link) for link in href]\n",
    "\n",
    "        # Add href to full_list\n",
    "        full_list.append(href)\n",
    "\n",
    "scraped_items = scrape_items(full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.pieces-euro.tv/allemagne/pieces-euro-berlin-2023', 'https://www.pieces-euro.tv/autriche/pieces-euro-2023', 'https://www.pieces-euro.tv/croatie/pieces-euro-2023', 'https://www.pieces-euro.tv/espagne/pieces-euro-2023', 'https://www.pieces-euro.tv/finlande/pieces-euro-2023', 'https://www.pieces-euro.tv/france/pieces-euro-2023', 'https://www.pieces-euro.tv/italie/pieces-euro-2023', 'https://www.pieces-euro.tv/lituanie/pieces-euro-2023', 'https://www.pieces-euro.tv/luxembourg/pieces-euro-2023', 'https://www.pieces-euro.tv/pays-bas/pieces-euro-2023', 'https://www.pieces-euro.tv/portugal/pieces-euro-2023', 'https://www.pieces-euro.tv/saint-marin/pieces-euro-2023', 'https://www.pieces-euro.tv/slovaquie/pieces-euro-2023', 'https://www.pieces-euro.tv/vatican/pieces-euro-2023', 'https://www.pieces-euro.tv/pieces-2-euros/2023', 'https://www.pieces-euro.tv/allemagne/pieces-euro-berlin-2023', 'https://www.pieces-euro.tv/autriche/pieces-euro-2023', 'https://www.pieces-euro.tv/croatie/pieces-euro-2023', 'https://www.pieces-euro.tv/espagne/pieces-euro-2023', 'https://www.pieces-euro.tv/finlande/pieces-euro-2023', 'https://www.pieces-euro.tv/france/pieces-euro-2023', 'https://www.pieces-euro.tv/italie/pieces-euro-2023', 'https://www.pieces-euro.tv/lituanie/pieces-euro-2023', 'https://www.pieces-euro.tv/luxembourg/pieces-euro-2023', 'https://www.pieces-euro.tv/pays-bas/pieces-euro-2023', 'https://www.pieces-euro.tv/portugal/pieces-euro-2023', 'https://www.pieces-euro.tv/saint-marin/pieces-euro-2023', 'https://www.pieces-euro.tv/slovaquie/pieces-euro-2023', 'https://www.pieces-euro.tv/vatican/pieces-euro-2023', 'https://www.pieces-euro.tv/pieces-2-euros/2023']\n"
     ]
    }
   ],
   "source": [
    "# Verify that coins froms 2023 are in the list\n",
    "urls_with_2023 = [url for sublist in full_list for url in sublist if \"2023\" in url]\n",
    "print(urls_with_2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a dataframe\n",
    "results_df = pd.DataFrame(scraped_items)\n",
    "results_df.head()\n",
    "\n",
    "# save the results in a parquet file for later use \n",
    "results_df.to_parquet('results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access to all the picture_url in the results list and save all the url in a list\n",
    "picture_urls = [item['picture_url'] for item in scraped_items]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to download coins pictures locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n"
     ]
    }
   ],
   "source": [
    "def download_images(picture_urls, folder_name):\n",
    "    \"\"\"\n",
    "    Downloads images from a list of picture URLs and saves them to a specified folder.\n",
    "\n",
    "    Args:\n",
    "        picture_urls (list): A list of picture URLs.\n",
    "        folder_name (str): The name of the folder to save the images in.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    for index, picture_url in enumerate(picture_urls):\n",
    "        if not picture_url or picture_url == 'No picture found' or len(picture_url) <= 20:\n",
    "            print(\"Invalid URL. Skipping to the next one.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(picture_url)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                filename = picture_url.split('/')[-1]\n",
    "                file_path = os.path.join(folder_name, filename)\n",
    "                with open(file_path, 'wb') as file:\n",
    "                    file.write(response.content)\n",
    "\n",
    "                #print(\"Image downloaded and saved:\", file_path)\n",
    "            else:\n",
    "                print(\"Failed to download the image:\", picture_url)\n",
    "        except requests.exceptions.MissingSchema:\n",
    "            print(\"Invalid URL. Skipping to the next one.\")\n",
    "            continue\n",
    "\n",
    "download_images(picture_urls, 'images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similiar process of Webscraping for valuable 2 euros coins in the same website\n",
    "\n",
    "* The code loops through each element in the HTML with the class `cat_itembox`,it extracts information about the coins, and stores it in a dictionary. \n",
    "The code appends each dictionary to a list, and then creates a pandas dataframe from the list. \n",
    "\n",
    "* The resulting dataframe contains information about the price, title, tirage, and picture URL of each valuable 2 euro coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>title</th>\n",
       "      <th>tirage</th>\n",
       "      <th>picture_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4179</td>\n",
       "      <td>Monaco 2 Euro commémorative 2007 - 25e anniver...</td>\n",
       "      <td>Tirage: 20.001  BU dans coffret original | On ...</td>\n",
       "      <td>https://www.pieces-euro.tv/img05/thumb/Monaco-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3837</td>\n",
       "      <td>Monaco 2 Euro commémorative 2015 - 800e annive...</td>\n",
       "      <td>Tirage: 10.000 | BE dans coffret original</td>\n",
       "      <td>https://www.pieces-euro.tv/img01/thumb/Monaco-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2076</td>\n",
       "      <td>Lituanie 2 Euro - UNESCO - Réserve biosphériqu...</td>\n",
       "      <td>Tirage: 500  (approx.) | BU en coincard | Au l...</td>\n",
       "      <td>https://www.pieces-euro.tv/img02/thumb/Lituani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1483</td>\n",
       "      <td>Luxembourg Série 2 Euro commémoratives 2008 - ...</td>\n",
       "      <td>Tirage: 2.500  BE | contient les 6 x 2 Euro co...</td>\n",
       "      <td>https://www.pieces-euro.tv/img03/thumb/Luxembo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1176</td>\n",
       "      <td>Pays-Bas 2 Euro commémorative 2015 - 30 ans du...</td>\n",
       "      <td>Tirage: 1.000 | contient 4 x 2 euro commémorat...</td>\n",
       "      <td>https://www.pieces-euro.tv/img05/thumb/Pays-Ba...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  price                                              title  \\\n",
       "0  4179  Monaco 2 Euro commémorative 2007 - 25e anniver...   \n",
       "1  3837  Monaco 2 Euro commémorative 2015 - 800e annive...   \n",
       "2  2076  Lituanie 2 Euro - UNESCO - Réserve biosphériqu...   \n",
       "3  1483  Luxembourg Série 2 Euro commémoratives 2008 - ...   \n",
       "4  1176  Pays-Bas 2 Euro commémorative 2015 - 30 ans du...   \n",
       "\n",
       "                                              tirage  \\\n",
       "0  Tirage: 20.001  BU dans coffret original | On ...   \n",
       "1          Tirage: 10.000 | BE dans coffret original   \n",
       "2  Tirage: 500  (approx.) | BU en coincard | Au l...   \n",
       "3  Tirage: 2.500  BE | contient les 6 x 2 Euro co...   \n",
       "4  Tirage: 1.000 | contient 4 x 2 euro commémorat...   \n",
       "\n",
       "                                         picture_url  \n",
       "0  https://www.pieces-euro.tv/img05/thumb/Monaco-...  \n",
       "1  https://www.pieces-euro.tv/img01/thumb/Monaco-...  \n",
       "2  https://www.pieces-euro.tv/img02/thumb/Lituani...  \n",
       "3  https://www.pieces-euro.tv/img03/thumb/Luxembo...  \n",
       "4  https://www.pieces-euro.tv/img05/thumb/Pays-Ba...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_valuables ='https://www.pieces-euro.tv/pieces-2-euros-valorisees'\n",
    "r = requests.get(url_valuables)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "list = []\n",
    "\n",
    "for element in soup.find_all(class_='cat_itembox'):\n",
    "    tirage = element.find('div', class_='cat_info')\n",
    "    # get the first element inside tirage\n",
    "    #tirage = tirage.find('p')\n",
    "    picture_div = element.find('div', class_='cat_preuser')\n",
    "\n",
    "    if picture_div:\n",
    "        img = picture_div.find('img')\n",
    "        if img:\n",
    "            picture_url = img['data-original']\n",
    "            base = 'https://www.pieces-euro.tv'\n",
    "            picture_url = base + picture_url\n",
    "        # check if the picture_url is valid and does not finish by \"noimage.png\"\n",
    "            if picture_url.endswith('noimage.png'):\n",
    "                picture_url = 'No picture found'\n",
    "        else:\n",
    "            picture_url = 'No picture found'\n",
    "\n",
    "\n",
    "    \n",
    "    for cat_catbox in element.find_all(class_='cat_catbox'):\n",
    "        # get the elements inside price\n",
    "        price_tag = cat_catbox.find('p', class_='cat_preis')\n",
    "        price = price_tag.get_text().split(',')[0]\n",
    "        \n",
    "        title = cat_catbox.find('p', class_='cat_titel')\n",
    "        title = title.text.strip() if title else 'No title found'\n",
    "        \n",
    "        \n",
    "        # Extract the text from the elements\n",
    "        tirage_text = tirage.text.strip() if tirage else 'No tirage found'\n",
    "\n",
    "        \n",
    "        # Store the information in a dictionary\n",
    "        item_info = {\n",
    "            'price': price,\n",
    "            'title': title,\n",
    "            'tirage': tirage_text,\n",
    "            'picture_url': picture_url,\n",
    "        }\n",
    "        \n",
    "        list.append(item_info)\n",
    "\n",
    "# Create a pandas dataframe from the results list\n",
    "two_euros_df = pd.DataFrame(list)\n",
    "two_euros_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results in a parquet file for later use\n",
    "\n",
    "two_euros_df.to_parquet('two_euros_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n",
      "Invalid URL. Skipping to the next one.\n"
     ]
    }
   ],
   "source": [
    "picture_urls = [item['picture_url'] for item in list]\n",
    "download_images(picture_urls, 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similiar Process for another data source website https://www.florinus.lt/numismatics/euro-coins/?sort_by=time_created\n",
    "TO DO : Use selenium to webscrape the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_valuables ='https://www.florinus.lt/numismatics/euro-coins'\n",
    "r = requests.get(url_valuables)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "list = []\n",
    "\n",
    "# get the div with the class 'item dynamic_products'\n",
    "\n",
    "main_dic = soup.find(class_='list_products')\n",
    "# \n",
    "\n",
    "# find all the class name 'name' inside the class 'list_products_slide active'\n",
    "\n",
    "#name = show.find(class_='name')\n",
    "\n",
    "#show = show.find_all(class_='name')\n",
    "    \n",
    "    #names = element.find('a', class_='name')\n",
    "    #name = names.text.strip() if names else 'No name found'\n",
    "    # store name in a list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the classes in soup and display them in a list\n",
    "\n",
    "main_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     if picture_div:\n",
    "#         img = picture_div.find('img')\n",
    "#         if img:\n",
    "#             picture_url = img['data-original']\n",
    "#             base = 'https://www.pieces-euro.tv'\n",
    "#             picture_url = base + picture_url\n",
    "#         # check if the picture_url is valid and does not finish by \"noimage.png\"\n",
    "#             if picture_url.endswith('noimage.png'):\n",
    "#                 picture_url = 'No picture found'\n",
    "#         else:\n",
    "#             picture_url = 'No picture found'\n",
    "\n",
    "\n",
    "    \n",
    "#     for cat_catbox in element.find_all(class_='cat_catbox'):\n",
    "#         # get the elements inside price\n",
    "#         price_tag = cat_catbox.find('p', class_='cat_preis')\n",
    "#         price = price_tag.get_text().split(',')[0]\n",
    "        \n",
    "#         title = cat_catbox.find('p', class_='cat_titel')\n",
    "#         title = title.text.strip() if title else 'No title found'\n",
    "        \n",
    "        \n",
    "#         # Extract the text from the elements\n",
    "#         tirage_text = tirage.text.strip() if tirage else 'No tirage found'\n",
    "\n",
    "        \n",
    "#         # Store the information in a dictionary\n",
    "#         item_info = {\n",
    "#             'price': price,\n",
    "#             'title': title,\n",
    "#             'tirage': tirage_text,\n",
    "#             'picture_url': picture_url,\n",
    "#         }\n",
    "        \n",
    "#         list.append(item_info)\n",
    "\n",
    "# # Create a pandas dataframe from the results list\n",
    "# two_euros_df = pd.DataFrame(list)\n",
    "# two_euros_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
